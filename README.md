[![LICENSE](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sebasmos/quantaudio/blob/main/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg)](https://github.com/sebasmos/quantaudio)

# QWave: Quantized Embeddings for Efficient Audio Classification

> ğŸš§ **This repository is under active development.**
>
> ğŸ“„ Code and models will be released upon preprint upload or journal submission.

---

## ğŸ” Overview

**QWave** provides an efficient and lightweight pipeline for soundscape classification based on quantized vector embeddings derived from pre-trained models. The framework supports ESC-50 and UrbanSound8K datasets and includes post-training quantization, cross-validation, and experiment tracking via Hydra.

---

## ğŸ“ Project Structure

```text
QuantAudio/
â”œâ”€â”€ configs/                   # Hydra configs for training and experiment tracking
â”‚   â””â”€â”€ configs.yaml           # Central configuration file
â”œâ”€â”€ QWave/                     # Core source code
â”‚   â”œâ”€â”€ datasets.py            # EmbeddingDataset class and quantization logic
â”‚   â”œâ”€â”€ models.py              # Simple MLP classifier definition
â”‚   â”œâ”€â”€ train_utils.py         # Training and logging utilities
â”‚   â”œâ”€â”€ memory.py              # Memory usage profiler
â”‚   â””â”€â”€ utils.py               # Save, seeding, and metric helpers
â”œâ”€â”€ scripts/                   # Run scripts
â”‚   â””â”€â”€ train_cv.py            # K-Fold cross-validation pipeline using Hydra
â”œâ”€â”€ outputs/                   # Auto-generated experiment results
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1. Create Environment

```bash
conda create -n qwave python=3.11 -y
conda activate qwave
```

### 2. Install Requirements

```bash


git clone https://github.com/sebasmos/qwave.git
cd qwave
pip install -e .
```

---

## ğŸš€ Run Cross-Validation For Vector Embeddings Framework

You can run an experiment with:

```bash
python run_trainer.py --config-name=esc50 \
  experiment.datasets.esc.csv=path_to_embeddings_csv \
  experiment.device=cpu \
```

For example: 

```bash
python run_trainer.py --config-name=esc50 \
  experiment.datasets.esc.csv=/Users/sebasmos/Documents/DATASETS/data_VE/ESC-50-master/VE_soundscapes/efficientnet_1536/esc-50.csv \
  experiment.device=cuda \
```

> âœ… This will save logs and checkpoints in `outputs/experiment_name/fold_*/`.

---

## ğŸ” Config Overview (`configs.yaml`)

```yaml
experiment:
  datasets:
    esc:
      csv: "/absolute/path/to/esc-50.csv"

  model:
    batch_size: 32
    hidden_sizes: [256, 128, 64]
    learning_rate: 0.001

  training:
    epochs: 50
    early_stopping:
      patience: 10
      delta: 0.01

  cross_validation:
    n_splits: 5
    shuffle: true
    random_seed: 42

  logging:
    log_interval: 50
    save_checkpoint: true
    resume: true

  metadata:
    tag: "exp01"
    notes: "EfficientNet baseline on ESC-50"
```

---


## â• Add a New Dataset

To add a new dataset configuration:

1. **Create a new YAML file** inside the `config/` folder. For example:

config/urbansound.yaml

2. **Define the structure** like this:

```yaml
defaults:
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

hydra:
  run:
    dir: ./outputs/${experiment.metadata.tag}
  output_subdir: null

experiment:
  device: cuda
  datasets:
    csv: /absolute/path/to/urbansound.csv
    imgs: /absolute/path/to/urbansound/images
  model:
    batch_size: 64
    hidden_sizes: [512, 256]
    learning_rate: 0.001
    dropout_prob: 0.3
    epochs: 100
    early_stopping:
      patience: 10
      delta: 0.01
    weight_decay: 0.001
    label_smoothing: 0.0
    patience: 10
    factor: 0.5
  cross_validation:
    n_splits: 5
    shuffle: true
    random_seed: 42
  logging:
    log_interval: 50
    save_checkpoint: true
    resume: true
  metadata:
    tag: urbansound_run
    notes: UrbanSound8K experiment
```

	3.	Run it using:

`python run_trainer.py --config-name=urbansound`


	4.	(Optional) Override fields at runtime:

```
python run_trainer.py --config-name=urbansound \
  experiment.datasets.csv=/custom/path/urbansound.csv \
  experiment.metadata.tag=my_custom_tag
```

--- 


## ğŸ“Š Features

- âœ… **Embedding extraction from EfficientNet / CLIP ViT**
- âœ… **Post-training quantization**
- âœ… **Cross-validation with reproducible config**
- âœ… **Class-imbalance handling**
- âœ… **Memory profiling & metrics logging**
- âœ… **Hydra integration for flexible experiments**

---

## ğŸ¤ Contributing

We welcome contributions! Fork the [repository](https://github.com/sebasmos/QuantAudio), make your improvements, and open a PR. Feature suggestions and bug reports are appreciated.

---

## ğŸ“„ License

This project is licensed under the [MIT License](https://github.com/sebasmos/QuantAudio/blob/main/LICENSE).

---

## ğŸ“™ Citation

```bibtex
@software{Cajas2025_QWave,
  author = {SebastiÃ¡n AndrÃ©s Cajas OrdÃ³Ã±ez and others},
  title = {QWave: Quantized Embeddings for Efficient Audio Classification},
  year = {2025},
  url = {https://github.com/sebasmos/QuantAudio},
  license = {MIT}
}
```

