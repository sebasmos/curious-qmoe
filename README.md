[![LICENSE](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sebasmos/quantaudio/blob/main/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg)](https://github.com/sebasmos/quantaudio)

# QWave: Quantized Embeddings for Efficient Audio Classification

> ğŸš§ **This repository is under active development.**
>
> ğŸ“„ Code and models will be released upon preprint upload or journal submission.

---

## ğŸ” Overview

**QWave** provides an efficient and lightweight pipeline for soundscape classification based on quantized vector embeddings derived from pre-trained models. The framework supports ESC-50 and UrbanSound8K datasets and includes post-training quantization, cross-validation, and experiment tracking via Hydra.

---

## ğŸ“ Project Structure

```text
QuantAudio/
â”œâ”€â”€ configs/                   # Hydra configs for training and experiment tracking
â”‚   â””â”€â”€ configs.yaml           # Central configuration file
â”œâ”€â”€ QWave/                     # Core source code
â”‚   â”œâ”€â”€ datasets.py            # EmbeddingDataset class and quantization logic
â”‚   â”œâ”€â”€ models.py              # Simple MLP classifier definition
â”‚   â”œâ”€â”€ train_utils.py         # Training and logging utilities
â”‚   â”œâ”€â”€ memory.py              # Memory usage profiler
â”‚   â””â”€â”€ utils.py               # Save, seeding, and metric helpers
â”œâ”€â”€ scripts/                   # Run scripts
â”‚   â””â”€â”€ train_cv.py            # K-Fold cross-validation pipeline using Hydra
â”œâ”€â”€ outputs/                   # Auto-generated experiment results
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md



â¸»

âš™ï¸ Setup

1. Create Environment

conda create -n qwave python=3.11 -y
conda activate qwave

2. Install Requirements

git clone https://github.com/sebasmos/QuantAudio.git
cd QuantAudio
pip install -r requirements.txt



â¸»

ğŸš€ Run Cross-Validation

You can run an experiment with:

python train_cv.py experiment.cross_validation.n_splits=5 \
                   experiment.model.batch_size=32 \
                   experiment.metadata.tag=exp01

âœ… This will save logs and checkpoints in outputs/exp01/fold_*/.

â¸»

ğŸ” Config Overview (configs.yaml)

experiment:
  datasets:
    esc:
      csv: "/absolute/path/to/esc-50.csv"

  model:
    batch_size: 32
    hidden_sizes: [256, 128, 64]
    learning_rate: 0.001

  training:
    epochs: 50
    early_stopping:
      patience: 10
      delta: 0.01

  cross_validation:
    n_splits: 5
    shuffle: true
    random_seed: 42

  logging:
    log_interval: 50
    save_checkpoint: true
    resume: true

  metadata:
    tag: "exp01"
    notes: "EfficientNet baseline on ESC-50"



â¸»

ğŸ“Š Features
	â€¢	âœ… Embedding extraction from EfficientNet / CLIP ViT
	â€¢	âœ… Post-training quantization
	â€¢	âœ… Cross-validation with reproducible config
	â€¢	âœ… Class-imbalance handling
	â€¢	âœ… Memory profiling & metrics logging
	â€¢	âœ… Hydra integration for flexible experiments

â¸»

ğŸ¤ Contributing

We welcome contributions! Fork the repository, make your improvements, and open a PR. Feature suggestions and bug reports are appreciated.

â¸»

ğŸ“„ License

This project is licensed under the MIT License.

â¸»

ğŸ“š Citation

@software{Cajas2025_QWave,
  author = {SebastiÃ¡n AndrÃ©s Cajas OrdÃ³Ã±ez and others},
  title = {QWave: Quantized Embeddings for Efficient Audio Classification},
  year = {2025},
  url = {https://github.com/sebasmos/QuantAudio},
  license = {MIT}
}
