[![LICENSE](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sebasmos/quantaudio/blob/main/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg)](https://github.com/sebasmos/quantaudio)

# QWave: Quantized Embeddings for Efficient Audio Classification

> üöß **This repository is under active development.**
>
> üìÑ Code and models will be released upon preprint upload or journal submission.

---

## üîç Overview

**QWave** provides an efficient and lightweight pipeline for soundscape classification based on quantized vector embeddings derived from pre-trained models. The framework supports:

- **Multiple quantization schemes**: 1, 2, 4, 8, 16-bit, BitNet ternary, and bitwise popcount
- **Mixture-of-Experts (MoE)** with heterogeneous quantized experts
- **Bayesian Router with curiosity mode** for epistemic uncertainty estimation
- **Accurate model size calculation** for quantized models
- **Cross-validation and experiment tracking** via Hydra

**Datasets**: ESC-50 and UrbanSound8K

**Requirements**: CUDA 12.6+ (optional for GPU acceleration)

---

## üìÅ Project Structure

```text
QWave/
‚îú‚îÄ‚îÄ config/                    # Hydra configs for experiments
‚îÇ   ‚îî‚îÄ‚îÄ esc50.yaml             # ESC-50 configuration with curiosity mode
‚îú‚îÄ‚îÄ QWave/                     # Core source code
‚îÇ   ‚îú‚îÄ‚îÄ datasets.py            # EmbeddingDataset class and normalization
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Neural network architectures (MLP, ESCModel)
‚îÇ   ‚îú‚îÄ‚îÄ bitnnet.py             # BitNet quantized layers (1-16 bit, ternary)
‚îÇ   ‚îú‚îÄ‚îÄ qmoe_layers.py         # Quantized MoE layers (BitwisePopcount)
‚îÇ   ‚îú‚îÄ‚îÄ moe.py                 # MoE training and Bayesian Router with curiosity
‚îÇ   ‚îú‚îÄ‚îÄ train_utils.py         # Training and validation utilities
‚îÇ   ‚îú‚îÄ‚îÄ memory.py              # Model size calculation (quantization-aware)
‚îÇ   ‚îú‚îÄ‚îÄ graphics.py            # Plotting (ROC, losses, curiosity distributions)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Helpers (seeding, device selection, metrics)
‚îú‚îÄ‚îÄ scripts/                   # Benchmark scripts
‚îÇ   ‚îî‚îÄ‚îÄ benchmark.py           # Main benchmarking pipeline with MoE support
‚îú‚îÄ‚îÄ outputs/                   # Auto-generated experiment results
‚îú‚îÄ‚îÄ pyproject.toml             # Package configuration
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Setup

### 1. Create Environment

```bash
conda create -n qwave python=3.11 -y
conda activate qwave
```

### 2. Install Requirements

```bash
git clone https://github.com/sebasmos/qwave.git
cd qwave
pip install -e .
```

---

## üöÄ Quick Start

### Basic Usage

Run benchmarks with the default configuration:

```bash
cd scripts
python benchmark.py \
  --config-path /path/to/QWave/config \
  --config-name esc50 \
  experiment.datasets.esc.csv=/path/to/esc-50.csv \
  experiment.device=cpu \
  experiment.models_to_run=[esc]
```

### MoE with Curiosity Mode

Enable Bayesian Router with epistemic uncertainty estimation:

```bash
python benchmark.py \
  --config-path /path/to/QWave/config \
  --config-name esc50 \
  experiment.device=cpu \
  experiment.datasets.esc.csv=/path/to/esc-50.csv \
  experiment.datasets.esc.normalization_type=standard \
  experiment.models_to_run=[moe] \
  experiment.router.expert_quantizations="[bitnet,'1','2','4','8','16',qesc]" \
  experiment.router.num_experts=3 \
  experiment.router.top_k=1 \
  experiment.router.use_curiosity=true \
  experiment.metadata.tag=esc_moe_curiosity
```

**Curiosity outputs** (saved per fold in `outputs/esc_moe_curiosity/moe/fold_X/`):
- `curiosity_values.json` - Raw uncertainty values per sample
- `curiosity_histogram.png` - Distribution of epistemic uncertainty
- `curiosity_per_class.png` - Average uncertainty per predicted class

> ‚úÖ Results and checkpoints are saved in `outputs/<tag>/<model>/fold_*/`

---

## üîç Config Overview

The main configuration file is `config/esc50.yaml`. Key parameters:

```yaml
experiment:
  models_to_run: [esc]  # Options: esc, bitnet, moe, qmoe, 1, 2, 4, 8, 16, qesc
  device: "cpu"  # or "cuda", "mps"

  datasets:
    esc:
      csv: "/path/to/esc-50.csv"
      normalization_type: "standard"  # or "l2", "min_max", "raw"

  model:
    batch_size: 64
    hidden_sizes: [640, 320]
    learning_rate: 0.0005793146438537801
    dropout_prob: 0.1953403862875243
    epochs: 10

  router:  # For MoE models
    expert_quantizations: [1, 2, 4, 16]
    num_experts: 4
    top_k: 1
    use_curiosity: false  # Enable Bayesian Router with uncertainty
    load_balancing: true
    load_balancing_alpha: 1e-3

  cross_validation:
    n_splits: 5
    shuffle: true
    random_seed: 42

  metadata:
    tag: "experiment_name"
    notes: ""
```

---


## üìä Features

- ‚úÖ **Embedding extraction from EfficientNet / CLIP ViT**
- ‚úÖ **Post-training quantization (1, 2, 4, 8, 16-bit)**
- ‚úÖ **BitNet ternary quantization**
- ‚úÖ **Mixture-of-Experts (MoE) with quantized experts**
- ‚úÖ **Bayesian Router with curiosity-driven uncertainty estimation**
- ‚úÖ **Accurate model size calculation for quantized models**
- ‚úÖ **Cross-validation with reproducible config**
- ‚úÖ **Class-imbalance handling**
- ‚úÖ **Memory profiling & metrics logging**
- ‚úÖ **Hydra integration for flexible experiments**

---

## üî¨ Benchmarking

### Model Size Calculation

QWave includes accurate model size calculation that accounts for quantization. Unlike traditional approaches that save `state_dict()` (which stores dequantized float32 weights), our implementation calculates the **theoretical quantized size** based on bit-width:

```python
from QWave.memory import print_size_of_model

# Automatically detects quantization and reports accurate size
print_size_of_model(model, label="quantized_model")
# Output: model: quantized_model     Size (KB): 12.345 [quantized]
```

**Supported quantization schemes:**
- **1-bit to 16-bit**: Symmetric quantization with scale factors
- **BitNet**: Ternary weights {-1, 0, 1} with per-channel alpha scaling
- **qesc**: Bitwise popcount with 2-bit ternary encoding

### Running Benchmarks

#### 1. Quantized Models Baseline (GPU)

Run all baseline quantized models (1, 2, 4, 8, 16-bit) and full-precision ESC model on GPU:

```bash
CUDA_VISIBLE_DEVICES=1 python benchmark.py \
  --config-path /home/sebasmos/Desktop/QWave/config \
  --config-name esc50 \
  experiment.datasets.esc.normalization_type=standard \
  experiment.datasets.esc.csv=/home/sebasmos/Documents/DATASET/esc-50.csv \
  experiment.device=cuda \
  experiment.models_to_run="['1','2','4','8','16',esc]" \
  experiment.metadata.tag=benchmark_baselines
```

#### 2. BitNet Baseline (CPU)

Run BitNet ternary quantization and full-precision models on CPU:

```bash
python benchmark.py \
  --config-path /home/sebasmos/Desktop/QWave/config \
  --config-name esc50 \
  experiment.datasets.esc.normalization_type=standard \
  experiment.datasets.esc.csv=/home/sebasmos/Documents/DATASET/esc-50.csv \
  experiment.device=cpu \
  experiment.models_to_run="[bitnet,esc]" \
  experiment.metadata.tag=benchmark_baselines
```

#### 3. Mixture-of-Experts (MoE)

Run MoE with heterogeneous quantized experts (BitNet, 1-bit, 2-bit, 4-bit, 8-bit, 16-bit, qesc):

```bash
python benchmark.py \
  --config-path /home/sebasmos/Desktop/QWave/config \
  --config-name esc50 \
  experiment.device=cpu \
  experiment.datasets.esc.csv=/home/sebasmos/Documents/DATASET/esc-50.csv \
  experiment.datasets.esc.normalization_type=standard \
  experiment.models_to_run=[moe] \
  experiment.router.expert_quantizations="[bitnet,'1','2','4','8','16',qesc]" \
  experiment.router.num_experts=3 \
  experiment.router.top_k=1 \
  experiment.metadata.tag=esc_moe_bitnet_8_16_qesc
```

#### 4. MoE with Curiosity (Bayesian Router)

Enable **curiosity mode** to use a Bayesian Router with Monte Carlo Dropout for epistemic uncertainty estimation:

```bash
python benchmark.py \
  --config-path /home/sebasmos/Desktop/QWave/config \
  --config-name esc50 \
  experiment.device=cpu \
  experiment.datasets.esc.csv=/home/sebasmos/Documents/DATASET/esc-50.csv \
  experiment.datasets.esc.normalization_type=standard \
  experiment.models_to_run=[moe] \
  experiment.router.expert_quantizations="[bitnet,'1','2','4','8','16',qesc]" \
  experiment.router.num_experts=3 \
  experiment.router.top_k=1 \
  experiment.router.use_curiosity=true \
  experiment.metadata.tag=esc_moe_bitnet_8_16_qesc_curiosity
```

**Curiosity outputs** (saved per fold):
- `curiosity_values.json` - Raw uncertainty values per sample
- `curiosity_histogram.png` - Distribution of epistemic uncertainty
- `curiosity_per_class.png` - Average uncertainty per predicted class

---

## ü§ù Contributing

We welcome contributions! Fork the [repository](https://github.com/sebasmos/QuantAudio), make your improvements, and open a PR. Feature suggestions and bug reports are appreciated.

---

## üìÑ License

This project is licensed under the [MIT License](https://github.com/sebasmos/QuantAudio/blob/main/LICENSE).

---

## üìô Citation

```bibtex
@software{Cajas2025_QWave,
  author = {Cajas-Ord√≥√±ez, Sebasti√°n Andr√©s and Torres, Luis and Meno, Mackenzie and Lai, Yuan and Dur√°n, Carlos and Celi, Leo Anthony},
  title = {QWave: Quantized Audio Classification Framework},
  year = {2025},
  url = {https://github.com/sebasmos/QWave},
  license = {MIT}
}
```
