{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b2b5-04a4-4a7f-b5ad-19ff4a30207f",
   "metadata": {},
   "source": [
    "# Vector Embedding Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072bae-5600-4f7c-84e4-1fc4e5a197c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "`data_path` is assumed to have the splitted data named as \"train/\", \"val/\" and \"test/\"\n",
    "\n",
    "**SUPORTED MODELs**\n",
    "\n",
    "\n",
    "- resnet50: 1536 h.u\n",
    "- mobilenet_v3_large\n",
    "- mobilenetv4_r448_trained\n",
    "- mobilenetv4_r448\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5878001-9b8c-4d3d-a936-4d06235a34de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = \"efficientnet_b3\"#\"resnet50\"\n",
    "batch_sizes = [64]\n",
    "embedding_sizes = [1536]#[2,4,8,64,1536]\n",
    "# data_path = '/home/sebastian/codes/data/UrbanSound8K/Mels_splitted'\n",
    "# data_path = '/home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds'\n",
    "data_path = '/Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_HD'\n",
    "# data_path = '../data/UrbanSound8K/Mels'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cb0a90-c2ad-4a89-baf3-3119236aab9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "MODEL_CONSTRUCTORS = {\n",
    "    'eva02_large_patch14_448_embeddings_imageNet':timm.create_model('eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', pretrained=True, num_classes=0),\n",
    "    'mobilenetv4_r448_trained': timm.create_model('mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', pretrained=False, num_classes=0),\n",
    "    'mobilenetv4_r448': timm.create_model('mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', pretrained=True, num_classes=0),\n",
    "    'resnet50':models.resnet50,\n",
    "    'alexnet': models.alexnet,\n",
    "    'convnext_base': models.convnext_base,\n",
    "    'convnext_large': models.convnext_large,\n",
    "    'convnext_small': models.convnext_small,\n",
    "    'convnext_tiny': models.convnext_tiny,\n",
    "    'densenet121': models.densenet121,\n",
    "    'densenet161': models.densenet161,\n",
    "    'densenet169': models.densenet169,\n",
    "    'densenet201': models.densenet201,\n",
    "    'efficientnet_b0': models.efficientnet_b0,\n",
    "    'efficientnet_b1': models.efficientnet_b1,\n",
    "    'efficientnet_b2': models.efficientnet_b2,\n",
    "    'efficientnet_b3': models.efficientnet_b3,\n",
    "    'efficientnet_b4': models.efficientnet_b4,\n",
    "    'efficientnet_b5': models.efficientnet_b5,\n",
    "    'efficientnet_b6': models.efficientnet_b6,\n",
    "    'efficientnet_b7': models.efficientnet_b7,\n",
    "    'efficientnet_v2_l': models.efficientnet_v2_l,\n",
    "    'efficientnet_v2_m': models.efficientnet_v2_m,\n",
    "    'efficientnet_v2_s': models.efficientnet_v2_s,\n",
    "    'googlenet': models.googlenet,\n",
    "    'inception_v3': models.inception_v3,\n",
    "    'maxvit_t': models.maxvit_t,\n",
    "    'mnasnet0_5': models.mnasnet0_5,\n",
    "    'mnasnet0_75': models.mnasnet0_75,\n",
    "    'mnasnet1_0': models.mnasnet1_0,\n",
    "    'mnasnet1_3': models.mnasnet1_3,\n",
    "    'mobilenet_v2': models.mobilenet_v2,\n",
    "    'mobilenet_v3_large': models.mobilenet_v3_large,\n",
    "    'mobilenet_v3_small': models.mobilenet_v3_small,\n",
    "    'regnet_x_16gf': models.regnet_x_16gf,\n",
    "    'regnet_x_1_6gf': models.regnet_x_1_6gf,\n",
    "    'regnet_x_32gf': models.regnet_x_32gf,\n",
    "    'regnet_x_3_2gf': models.regnet_x_3_2gf,\n",
    "    'regnet_x_400mf': models.regnet_x_400mf,\n",
    "    'regnet_x_800mf': models.regnet_x_800mf,\n",
    "    'regnet_x_8gf': models.regnet_x_8gf,\n",
    "    'regnet_y_128gf': models.regnet_y_128gf,# check this regnet_y_128gf: no weigthts avaialble\n",
    "    'regnet_y_16gf': models.regnet_y_16gf,\n",
    "    'regnet_y_1_6gf': models.regnet_y_1_6gf,\n",
    "    'regnet_y_32gf': models.regnet_y_32gf,\n",
    "    'regnet_y_3_2gf': models.regnet_y_3_2gf,\n",
    "    'regnet_y_400mf': models.regnet_y_400mf,\n",
    "    'regnet_y_800mf': models.regnet_y_800mf,\n",
    "    'regnet_y_8gf': models.regnet_y_8gf,\n",
    "    'resnet101': models.resnet101,\n",
    "    'resnet152': models.resnet152,\n",
    "    'resnet18': models.resnet18,\n",
    "    'resnet34': models.resnet34,\n",
    "    'resnet50': models.resnet50,\n",
    "    'resnext101_32x8d': models.resnext101_32x8d,\n",
    "    'resnext101_64x4d': models.resnext101_64x4d,\n",
    "    'resnext50_32x4d': models.resnext50_32x4d,\n",
    "    'shufflenet_v2_x0_5': models.shufflenet_v2_x0_5,\n",
    "    'shufflenet_v2_x1_0': models.shufflenet_v2_x1_0,\n",
    "    'shufflenet_v2_x1_5': models.shufflenet_v2_x1_5,\n",
    "    'shufflenet_v2_x2_0': models.shufflenet_v2_x2_0,\n",
    "    'squeezenet1_0': models.squeezenet1_0,\n",
    "    'squeezenet1_1': models.squeezenet1_1,\n",
    "    'swin_b': models.swin_b,\n",
    "    'swin_s': models.swin_s,\n",
    "    'swin_t': models.swin_t,\n",
    "    'swin_v2_b': models.swin_v2_b,\n",
    "    'swin_v2_s': models.swin_v2_s,\n",
    "    'swin_v2_t': models.swin_v2_t,\n",
    "    'vgg11': models.vgg11,\n",
    "    'vgg11_bn': models.vgg11_bn,\n",
    "    'vgg13': models.vgg13,\n",
    "    'vgg13_bn': models.vgg13_bn,\n",
    "    'vgg16': models.vgg16,\n",
    "    'vgg16_bn': models.vgg16_bn,\n",
    "    'vgg19': models.vgg19,\n",
    "    'vgg19_bn': models.vgg19_bn,\n",
    "    'vit_b_16': models.vit_b_16,\n",
    "    'vit_b_32': models.vit_b_32,\n",
    "    'vit_h_14': models.vit_h_14,# and this..no weigthts avaialble\n",
    "    'vit_l_16': models.vit_l_16,\n",
    "    'vit_l_32': models.vit_l_32,\n",
    "    'wide_resnet101_2': models.wide_resnet101_2,\n",
    "    'wide_resnet50_2': models.wide_resnet50_2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c58f-d275-4b39-82cb-9ec06349b322",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370eb3e4-97f2-401c-83ba-b849b83761c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "CUDA version: None - Torch versteion: 2.4.0 - device count: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0,'../') \n",
    "sys.path.insert(0,'../../') \n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd \n",
    "# from MAE code\n",
    "from util.datasets import build_dataset\n",
    "import argparse\n",
    "import util.misc as misc\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# import models_vit\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch; print(f'numpy version: {np.__version__}\\nCUDA version: {torch.version.cuda} - Torch versteion: {torch.__version__} - device count: {torch.cuda.device_count()}')\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import sys\n",
    "sys.path.insert(0,'../../')\n",
    "from qnet import *\n",
    "mp.set_sharing_strategy('file_system')\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def create_args(batch_size, model_name, embedding_size, output_dir, data_path, device):\n",
    "    parser = argparse.ArgumentParser('VE extraction', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=batch_size, help='Batch size per GPU')\n",
    "    parser.add_argument('--embedding_size', default=embedding_size, help='embedding_size')\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--accum_iter', default=4, type=int,\n",
    "                        help='Accumulate gradient iterations')\n",
    "    parser.add_argument('--model', default=model_name, type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--data_path', default=data_path, type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--nb_classes', default=5, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--output_dir', default=output_dir,\n",
    "                        help='path where to save')\n",
    "    parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default=device,\n",
    "                        help='device to use for training/testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "        # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=None, metavar='PCT',\n",
    "                            help='Color jitter factor (enabled only when not using Auto/RandAug)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                            help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                            help='Label smoothing (default: 0.1)')\n",
    "        # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                            help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                            help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                            help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                            help='Do not random erase first (clean) augmentation split')\n",
    "        # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                            help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                            help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                            help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                            help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                            help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                            help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "    parser.add_argument('--resume', default=\".\",\n",
    "                        help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', default=True, action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c373766-2861-4dd6-b3da-90b1675a0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls '/home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585d9a1-7586-414a-ad85-8ebac0172f9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image Vector Embeddings Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc75ff2-059d-475a-8edf-520c12cb9d39",
   "metadata": {},
   "source": [
    "### Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0bcfa57-bcad-40be-b1be-bb468113e4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "efficientnet_b3_1536_bs64-----------------\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_HD/train\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_HD/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m fold_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m args \u001b[38;5;241m=\u001b[39m create_args(batch_size, model_name, embedding_size, model_name, fold_dir, device)\n\u001b[1;32m     64\u001b[0m model, preprocess, transforms, data_config \u001b[38;5;241m=\u001b[39m initialize_model(args\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m.\u001b[39membedding_size, MODEL_CONSTRUCTORS)\n",
      "File \u001b[0;32m~/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_HD/train.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_embeddings_ESC(model, data_loader, experiment_name, fold, device, preprocess=None, transforms=None):\n",
    "    embeddings_list = []\n",
    "    targets_list = []\n",
    "    total_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad(), tqdm(total=total_batches) as pbar:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        model.to(device)\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            if preprocess:\n",
    "                images = preprocess(images).squeeze()\n",
    "                images = images.to(device)\n",
    "                embeddings = model(images)\n",
    "            elif transforms:  # for timm models\n",
    "                images = images.to(device)   \n",
    "                embeddings = model(transforms(images))  # output is (batch_size, num_features) shaped tensor\n",
    "            else:\n",
    "                images = images.to(device)\n",
    "                embeddings = model(images)\n",
    "            \n",
    "            embeddings_list.append(embeddings.cpu().detach().numpy())  # Move to CPU and convert to NumPy\n",
    "            targets_list.append(targets.numpy())  # Convert targets to NumPy\n",
    "            pbar.update(1)\n",
    "    # Concatenate embeddings and targets from all batches\n",
    "    embeddings = np.concatenate(embeddings_list).squeeze()\n",
    "    targets = np.concatenate(targets_list)\n",
    "    num_embeddings = embeddings.shape[1]\n",
    "    column_names = [f\"feat_{i}\" for i in range(num_embeddings)]\n",
    "    column_names.append(\"label\")\n",
    "\n",
    "    embeddings_with_targets = np.hstack((embeddings, np.expand_dims(targets, axis=1)))\n",
    "\n",
    "    # Create a DataFrame with column names\n",
    "    df = pd.DataFrame(embeddings_with_targets, columns=column_names)\n",
    "    \n",
    "    df.to_csv(os.path.join(experiment_name, f\"{str(fold)}.csv\"), index=False)\n",
    "    return embeddings_with_targets\n",
    "\n",
    "# Assuming data_path is already defined\n",
    "folds = sorted([d for d in os.listdir(data_path) if not \"csv\" in d and not '.ipynb_checkpoints' in d])\n",
    "\n",
    "for embedding_size in embedding_sizes:\n",
    "    for batch_size in batch_sizes:\n",
    "        experiment_name = f\"{model_name}_{embedding_size}_bs{batch_size}\"\n",
    "        os.makedirs(experiment_name, exist_ok=True)\n",
    "        print(f\"\\n{model_name}_{embedding_size}_bs{batch_size}\".center(60, \"-\"))\n",
    "        total_embeddings = 0\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_dir = os.path.join(data_path, fold)\n",
    "            print(f\"Processing {fold_dir}\")\n",
    "            \n",
    "            csv_file_path = os.path.join(data_path, f\"{fold}.csv\")\n",
    "            fold_data = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            args = create_args(batch_size, model_name, embedding_size, model_name, fold_dir, device)\n",
    "            model, preprocess, transforms, data_config = initialize_model(args.model, args.embedding_size, MODEL_CONSTRUCTORS)\n",
    "            print(data_config, transforms)\n",
    "            \n",
    "            dataloader = build_dataset_fold(args, fold_dir, fold_data)\n",
    "\n",
    "            output_csv_path = os.path.join(experiment_name, f\"{fold}_embedding.csv\")\n",
    "\n",
    "            embeddings = extract_embeddings_ESC(model, dataloader, experiment_name, fold, device, preprocess=preprocess, transforms=transforms)\n",
    "            print(embeddings.shape)\n",
    "\n",
    "print(\"All configurations processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd49f17-034b-4091-b9df-f728262b09e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b99af1e-7a42-4029-ac3b-9b3180e3257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/Documents/VE_paper/0_VE_extraction/part0_ESC-VE-extraction\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARF_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
