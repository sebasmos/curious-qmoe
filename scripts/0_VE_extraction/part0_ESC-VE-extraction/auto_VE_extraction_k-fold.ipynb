{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b2b5-04a4-4a7f-b5ad-19ff4a30207f",
   "metadata": {},
   "source": [
    "# Vector Embedding Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072bae-5600-4f7c-84e4-1fc4e5a197c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "`data_path` is assumed to have the splitted data named as \"train/\", \"val/\" and \"test/\"\n",
    "\n",
    "**SUPORTED MODELs**\n",
    "\n",
    "\n",
    "- resnet50: 1536 h.u\n",
    "- mobilenet_v3_large\n",
    "- mobilenetv4_r448_trained\n",
    "- mobilenetv4_r448\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5878001-9b8c-4d3d-a936-4d06235a34de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = \"efficientnet_b3\"#\"resnet50\"\n",
    "batch_sizes = [64]\n",
    "embedding_sizes = [1536]#[2,4,8,64,1536]\n",
    "# data_path = '/home/sebastian/codes/data/UrbanSound8K/Mels_splitted'\n",
    "# data_path = '/home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds'\n",
    "data_path = '/Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds'\n",
    "# data_path = '../data/UrbanSound8K/Mels'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cb0a90-c2ad-4a89-baf3-3119236aab9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "MODEL_CONSTRUCTORS = {\n",
    "    'eva02_large_patch14_448_embeddings_imageNet':timm.create_model('eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', pretrained=True, num_classes=0),\n",
    "    'mobilenetv4_r448_trained': timm.create_model('mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', pretrained=False, num_classes=0),\n",
    "    'mobilenetv4_r448': timm.create_model('mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', pretrained=True, num_classes=0),\n",
    "    'resnet50':models.resnet50,\n",
    "    'alexnet': models.alexnet,\n",
    "    'convnext_base': models.convnext_base,\n",
    "    'convnext_large': models.convnext_large,\n",
    "    'convnext_small': models.convnext_small,\n",
    "    'convnext_tiny': models.convnext_tiny,\n",
    "    'densenet121': models.densenet121,\n",
    "    'densenet161': models.densenet161,\n",
    "    'densenet169': models.densenet169,\n",
    "    'densenet201': models.densenet201,\n",
    "    'efficientnet_b0': models.efficientnet_b0,\n",
    "    'efficientnet_b1': models.efficientnet_b1,\n",
    "    'efficientnet_b2': models.efficientnet_b2,\n",
    "    'efficientnet_b3': models.efficientnet_b3,\n",
    "    'efficientnet_b4': models.efficientnet_b4,\n",
    "    'efficientnet_b5': models.efficientnet_b5,\n",
    "    'efficientnet_b6': models.efficientnet_b6,\n",
    "    'efficientnet_b7': models.efficientnet_b7,\n",
    "    'efficientnet_v2_l': models.efficientnet_v2_l,\n",
    "    'efficientnet_v2_m': models.efficientnet_v2_m,\n",
    "    'efficientnet_v2_s': models.efficientnet_v2_s,\n",
    "    'googlenet': models.googlenet,\n",
    "    'inception_v3': models.inception_v3,\n",
    "    'maxvit_t': models.maxvit_t,\n",
    "    'mnasnet0_5': models.mnasnet0_5,\n",
    "    'mnasnet0_75': models.mnasnet0_75,\n",
    "    'mnasnet1_0': models.mnasnet1_0,\n",
    "    'mnasnet1_3': models.mnasnet1_3,\n",
    "    'mobilenet_v2': models.mobilenet_v2,\n",
    "    'mobilenet_v3_large': models.mobilenet_v3_large,\n",
    "    'mobilenet_v3_small': models.mobilenet_v3_small,\n",
    "    'regnet_x_16gf': models.regnet_x_16gf,\n",
    "    'regnet_x_1_6gf': models.regnet_x_1_6gf,\n",
    "    'regnet_x_32gf': models.regnet_x_32gf,\n",
    "    'regnet_x_3_2gf': models.regnet_x_3_2gf,\n",
    "    'regnet_x_400mf': models.regnet_x_400mf,\n",
    "    'regnet_x_800mf': models.regnet_x_800mf,\n",
    "    'regnet_x_8gf': models.regnet_x_8gf,\n",
    "    'regnet_y_128gf': models.regnet_y_128gf,# check this regnet_y_128gf: no weigthts avaialble\n",
    "    'regnet_y_16gf': models.regnet_y_16gf,\n",
    "    'regnet_y_1_6gf': models.regnet_y_1_6gf,\n",
    "    'regnet_y_32gf': models.regnet_y_32gf,\n",
    "    'regnet_y_3_2gf': models.regnet_y_3_2gf,\n",
    "    'regnet_y_400mf': models.regnet_y_400mf,\n",
    "    'regnet_y_800mf': models.regnet_y_800mf,\n",
    "    'regnet_y_8gf': models.regnet_y_8gf,\n",
    "    'resnet101': models.resnet101,\n",
    "    'resnet152': models.resnet152,\n",
    "    'resnet18': models.resnet18,\n",
    "    'resnet34': models.resnet34,\n",
    "    'resnet50': models.resnet50,\n",
    "    'resnext101_32x8d': models.resnext101_32x8d,\n",
    "    'resnext101_64x4d': models.resnext101_64x4d,\n",
    "    'resnext50_32x4d': models.resnext50_32x4d,\n",
    "    'shufflenet_v2_x0_5': models.shufflenet_v2_x0_5,\n",
    "    'shufflenet_v2_x1_0': models.shufflenet_v2_x1_0,\n",
    "    'shufflenet_v2_x1_5': models.shufflenet_v2_x1_5,\n",
    "    'shufflenet_v2_x2_0': models.shufflenet_v2_x2_0,\n",
    "    'squeezenet1_0': models.squeezenet1_0,\n",
    "    'squeezenet1_1': models.squeezenet1_1,\n",
    "    'swin_b': models.swin_b,\n",
    "    'swin_s': models.swin_s,\n",
    "    'swin_t': models.swin_t,\n",
    "    'swin_v2_b': models.swin_v2_b,\n",
    "    'swin_v2_s': models.swin_v2_s,\n",
    "    'swin_v2_t': models.swin_v2_t,\n",
    "    'vgg11': models.vgg11,\n",
    "    'vgg11_bn': models.vgg11_bn,\n",
    "    'vgg13': models.vgg13,\n",
    "    'vgg13_bn': models.vgg13_bn,\n",
    "    'vgg16': models.vgg16,\n",
    "    'vgg16_bn': models.vgg16_bn,\n",
    "    'vgg19': models.vgg19,\n",
    "    'vgg19_bn': models.vgg19_bn,\n",
    "    'vit_b_16': models.vit_b_16,\n",
    "    'vit_b_32': models.vit_b_32,\n",
    "    'vit_h_14': models.vit_h_14,# and this..no weigthts avaialble\n",
    "    'vit_l_16': models.vit_l_16,\n",
    "    'vit_l_32': models.vit_l_32,\n",
    "    'wide_resnet101_2': models.wide_resnet101_2,\n",
    "    'wide_resnet50_2': models.wide_resnet50_2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c58f-d275-4b39-82cb-9ec06349b322",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370eb3e4-97f2-401c-83ba-b849b83761c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "CUDA version: None - Torch versteion: 2.4.0 - device count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0,'../') \n",
    "sys.path.insert(0,'../../') \n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd \n",
    "# from MAE code\n",
    "from util.datasets import build_dataset\n",
    "import argparse\n",
    "import util.misc as misc\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# import models_vit\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch; print(f'numpy version: {np.__version__}\\nCUDA version: {torch.version.cuda} - Torch versteion: {torch.__version__} - device count: {torch.cuda.device_count()}')\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import sys\n",
    "sys.path.insert(0,'../../')\n",
    "from qnet import *\n",
    "mp.set_sharing_strategy('file_system')\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def create_args(batch_size, model_name, embedding_size, output_dir, data_path, device):\n",
    "    parser = argparse.ArgumentParser('VE extraction', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=batch_size, help='Batch size per GPU')\n",
    "    parser.add_argument('--embedding_size', default=embedding_size, help='embedding_size')\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--accum_iter', default=4, type=int,\n",
    "                        help='Accumulate gradient iterations')\n",
    "    parser.add_argument('--model', default=model_name, type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--data_path', default=data_path, type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--nb_classes', default=5, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--output_dir', default=output_dir,\n",
    "                        help='path where to save')\n",
    "    parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default=device,\n",
    "                        help='device to use for training/testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "        # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=None, metavar='PCT',\n",
    "                            help='Color jitter factor (enabled only when not using Auto/RandAug)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                            help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                            help='Label smoothing (default: 0.1)')\n",
    "        # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                            help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                            help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                            help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                            help='Do not random erase first (clean) augmentation split')\n",
    "        # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                            help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                            help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                            help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                            help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                            help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                            help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "    parser.add_argument('--resume', default=\".\",\n",
    "                        help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', default=True, action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c373766-2861-4dd6-b3da-90b1675a0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls '/home/sebastian/codes/repo_clean/VE_paper/data/ESC-50-master/Mels_Folds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585d9a1-7586-414a-ad85-8ebac0172f9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image Vector Embeddings Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc75ff2-059d-475a-8edf-520c12cb9d39",
   "metadata": {},
   "source": [
    "### Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bcfa57-bcad-40be-b1be-bb468113e4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "efficientnet_b3_1536_bs64-----------------\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "Dataset built with 400 images from /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:10<00:00, 10.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1537)\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "Dataset built with 400 images from /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:09<00:00, 10.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1537)\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_3\n",
      "None None\n",
      "Dataset built with 400 images from /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 7/7 [01:11<00:00, 10.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1537)\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_4\n",
      "None None\n",
      "Dataset built with 400 images from /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 7/7 [01:11<00:00, 10.20s/it]\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebasmos/anaconda3/envs/ARF_paper/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1537)\n",
      "Processing /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_5\n",
      "None None\n",
      "Dataset built with 400 images from /Users/sebasmos/Documents/VE_paper/data/ESC-50-master/Mels_Folds/fold_5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:10<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1537)\n",
      "All configurations processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_embeddings_ESC(model, data_loader, experiment_name, fold, device, preprocess=None, transforms=None):\n",
    "    embeddings_list = []\n",
    "    targets_list = []\n",
    "    total_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad(), tqdm(total=total_batches) as pbar:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        model.to(device)\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            if preprocess:\n",
    "                images = preprocess(images).squeeze()\n",
    "                images = images.to(device)\n",
    "                embeddings = model(images)\n",
    "            elif transforms:  # for timm models\n",
    "                images = images.to(device)   \n",
    "                embeddings = model(transforms(images))  # output is (batch_size, num_features) shaped tensor\n",
    "            else:\n",
    "                images = images.to(device)\n",
    "                embeddings = model(images)\n",
    "            \n",
    "            embeddings_list.append(embeddings.cpu().detach().numpy())  # Move to CPU and convert to NumPy\n",
    "            targets_list.append(targets.numpy())  # Convert targets to NumPy\n",
    "            pbar.update(1)\n",
    "    # Concatenate embeddings and targets from all batches\n",
    "    embeddings = np.concatenate(embeddings_list).squeeze()\n",
    "    targets = np.concatenate(targets_list)\n",
    "    num_embeddings = embeddings.shape[1]\n",
    "    column_names = [f\"feat_{i}\" for i in range(num_embeddings)]\n",
    "    column_names.append(\"label\")\n",
    "\n",
    "    embeddings_with_targets = np.hstack((embeddings, np.expand_dims(targets, axis=1)))\n",
    "\n",
    "    # Create a DataFrame with column names\n",
    "    df = pd.DataFrame(embeddings_with_targets, columns=column_names)\n",
    "    \n",
    "    df.to_csv(os.path.join(experiment_name, f\"{str(fold)}.csv\"), index=False)\n",
    "    return embeddings_with_targets\n",
    "\n",
    "# Assuming data_path is already defined\n",
    "folds = sorted([d for d in os.listdir(data_path) if not \"csv\" in d and not '.ipynb_checkpoints' in d])\n",
    "\n",
    "for embedding_size in embedding_sizes:\n",
    "    for batch_size in batch_sizes:\n",
    "        experiment_name = f\"{model_name}_{embedding_size}_bs{batch_size}\"\n",
    "        os.makedirs(experiment_name, exist_ok=True)\n",
    "        print(f\"\\n{model_name}_{embedding_size}_bs{batch_size}\".center(60, \"-\"))\n",
    "        total_embeddings = 0\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_dir = os.path.join(data_path, fold)\n",
    "            print(f\"Processing {fold_dir}\")\n",
    "            \n",
    "            csv_file_path = os.path.join(data_path, f\"{fold}.csv\")\n",
    "            fold_data = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            args = create_args(batch_size, model_name, embedding_size, model_name, fold_dir, device)\n",
    "            model, preprocess, transforms, data_config = initialize_model(args.model, args.embedding_size, MODEL_CONSTRUCTORS)\n",
    "            print(data_config, transforms)\n",
    "            \n",
    "            dataloader = build_dataset_fold(args, fold_dir, fold_data)\n",
    "\n",
    "            output_csv_path = os.path.join(experiment_name, f\"{fold}_embedding.csv\")\n",
    "\n",
    "            embeddings = extract_embeddings_ESC(model, dataloader, experiment_name, fold, device, preprocess=preprocess, transforms=transforms)\n",
    "            print(embeddings.shape)\n",
    "\n",
    "print(\"All configurations processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd49f17-034b-4091-b9df-f728262b09e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b99af1e-7a42-4029-ac3b-9b3180e3257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sebasmos/Documents/VE_paper/0_VE_extraction/part0_ESC-VE-extraction\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
