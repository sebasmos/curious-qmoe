{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b2b5-04a4-4a7f-b5ad-19ff4a30207f",
   "metadata": {},
   "source": [
    "# Vector Embedding Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072bae-5600-4f7c-84e4-1fc4e5a197c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "`data_path` is assumed to have the splitted data named as \"train/\", \"val/\" and \"test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5878001-9b8c-4d3d-a936-4d06235a34de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "model_name = \"mobilenetv4_r448_pretrained\"#\"efficientnet_b3\" #EfficientNet_B7_Weights.IMAGENET1K_V1\n",
    "weights_path = '/home/sebastian/codes/QuantumVE/q_Net/pretrain/mobilenetv4_r448/checkpoint-99.pth'\n",
    "feat_space = 16\n",
    "batch_size = 64\n",
    "data_path = '../data/ABGQI_mel_spectrograms'\n",
    "device = 'cuda'\n",
    "output_dir = \"embeddings\"\n",
    "\n",
    "# Create experiment directory\n",
    "EXPERIMENT_NAME = f\"./{output_dir}/{model_name}_{feat_space}_bs{batch_size}\"\n",
    "import os\n",
    "os.makedirs(EXPERIMENT_NAME, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffab86-7be8-4712-8f86-37ca9ffc5447",
   "metadata": {},
   "source": [
    "# Load pre-trained model with Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69bef5ac-2c1f-44fc-b106-8b9bf820365c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:52.249011] [16:35:52.249008] [16:35:52.249067] [16:35:52.248985] [16:35:52.249076] [16:35:52.249074] [16:35:52.249082] Before loading checkpoint Trainable params: 31309864 of 31309864\n",
      "[16:35:52.384900] [16:35:52.384897] [16:35:52.384919] [16:35:52.384882] [16:35:52.384929] [16:35:52.384927] [16:35:52.384935] After loading checkpoint Trainable params: 31309864 of 31309864\n",
      "[16:35:52.402295] [16:35:52.402293] [16:35:52.402315] [16:35:52.402277] [16:35:52.402325] [16:35:52.402323] [16:35:52.402331] Custom model parameters Trainable params: 31316269 of 31316269\n",
      "[16:35:52.477369] [16:35:52.477366] [16:35:52.477414] [16:35:52.477352] [16:35:52.477423] [16:35:52.477421] [16:35:52.477428] Output features shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model, message=\"\"):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{message} Trainable params: {trainable_params} of {total_params}\")\n",
    "\n",
    "# Define extract_embeddings class\n",
    "class extract_embeddings(nn.Module):\n",
    "    def __init__(self, base_model, feat_space, model_name):\n",
    "        super(extract_embeddings, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.model_name = model_name\n",
    "        self.feat_space = feat_space\n",
    "        \n",
    "        # Example: Adding a new classifier layer\n",
    "        if model_name == \"mobilenetv4_r448_pretrained\":\n",
    "            self.new_classifier = nn.Linear(self.base_model.conv_head.out_channels, out_features=self.feat_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.new_classifier(x)\n",
    "        return x\n",
    "    \n",
    "def load_and_initialize_model(model_name, weights_path, feat_space):\n",
    "    model = timm.create_model('mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', pretrained=False, num_classes=0)\n",
    "\n",
    "    # Count parameters before loading the checkpoint\n",
    "    count_parameters(model, message=\"Before loading checkpoint\")\n",
    "\n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "    checkpoint_model = checkpoint['model']\n",
    "\n",
    "    # Count parameters after loading the checkpoint\n",
    "    count_parameters(model, message=\"After loading checkpoint\")\n",
    "\n",
    "    # Initialize the extract_embeddings with the base model and new classifier\n",
    "    model = extract_embeddings(base_model=model, feat_space=feat_space, model_name=model_name)\n",
    "    # Load updated checkpoint into the model\n",
    "    model.load_state_dict(checkpoint_model, strict=False)\n",
    "\n",
    "    # Count parameters of the custom model\n",
    "    count_parameters(model, message=\"Custom model parameters\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Pre-trained model with 5 classes so need to be initialized with with the same\n",
    "model = load_and_initialize_model(model_name, weights_path, 5)\n",
    "\n",
    "# Create a dummy input tensor (e.g., batch size 1, 3 channels, 448x448 image size)\n",
    "dummy_input = torch.randn(1, 3, 448, 448)  # Adjust size if necessary\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_features = model(dummy_input)\n",
    "print(\"Output features shape:\", output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c35f98-06cc-432d-a8a7-cee0aa541492",
   "metadata": {},
   "source": [
    "Adapt model for N embedding-sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6094b174-25fd-41b0-822c-2a51f5a9930d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:07:21.856092] [16:07:21.856076] [16:07:21.856151] Output features shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the extract_embeddings with the base model and new classifier\n",
    "model = extract_embeddings(base_model=model.base_model, feat_space=feat_space, model_name=model_name)\n",
    "# Create a dummy input tensor (e.g., batch size 1, 3 channels, 448x448 image size)\n",
    "dummy_input = torch.randn(1, 3, 448, 448)  # Adjust size if necessary\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output_features = model(dummy_input)\n",
    "\n",
    "# Print the shape of the output features\n",
    "print(\"Output features shape:\", output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ef064-886e-483d-a8a2-2f50025419eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc18e87-611c-4426-956e-5ffee42a074a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:07:23.217299] [16:07:23.217291] [16:07:23.217324] numpy version: 1.26.4\n",
      "CUDA version: 11.8 - Torch versteion: 2.0.0+cu118 - device count: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "from util.datasets import build_dataset\n",
    "import sys\n",
    "sys.path.insert(0,'../') \n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd \n",
    "# from MAE code\n",
    "from util.datasets import build_dataset\n",
    "import argparse\n",
    "import util.misc as misc\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# import models_vit\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch; print(f'numpy version: {np.__version__}\\nCUDA version: {torch.version.cuda} - Torch versteion: {torch.__version__} - device count: {torch.cuda.device_count()}')\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np\n",
    "\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def count_parameters(model, message=\"\"):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{message} Trainable params: {trainable_params} of {total_params}\")\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)()\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "\n",
    "def plot_multiclass_roc_curve(all_labels, all_predictions, EXPERIMENT_NAME=\".\"):\n",
    "    # Step 1: Label Binarization\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y_onehot = label_binarizer.fit_transform(all_labels)\n",
    "    all_predictions_hot = label_binarizer.transform(all_predictions)\n",
    "\n",
    "    # Step 2: Calculate ROC curves\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    unique_classes = range(y_onehot.shape[1])\n",
    "    for i in unique_classes:\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot[:, i], all_predictions_hot[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Step 3: Plot ROC curves\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_onehot.ravel(), all_predictions_hot.ravel())\n",
    "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "    plt.plot(\n",
    "        fpr_micro,\n",
    "        tpr_micro,\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc_micro:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in unique_classes]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in unique_classes:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= len(unique_classes)\n",
    "    fpr_macro = all_fpr\n",
    "    tpr_macro = mean_tpr\n",
    "    roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "    plt.plot(\n",
    "        fpr_macro,\n",
    "        tpr_macro,\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc_macro:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Individual class ROC curves with unique colors\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_classes)))\n",
    "    for class_id, color in zip(unique_classes, colors):\n",
    "        plt.plot(\n",
    "            fpr[class_id],\n",
    "            tpr[class_id],\n",
    "            color=color,\n",
    "            label=f\"ROC curve for Class {class_id} (AUC = {roc_auc[class_id]:.2f})\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)  # Add diagonal line for reference\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Extension of Receiver Operating Characteristic\\n to One-vs-Rest multiclass\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{EXPERIMENT_NAME}/roc_curve.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa06ad-a5d6-465d-ab7e-22ae60da1ec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parametrize and initialize seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0103a95a-8f91-47fd-8420-d5d769fd77de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:07:24.254396] [16:07:24.254382] [16:07:24.254453] Namespace(batch_size=64,\n",
      "epochs=50,\n",
      "accum_iter=4,\n",
      "model='mobilenetv4_r448_pretrained',\n",
      "input_size=224,\n",
      "drop_path=0.1,\n",
      "clip_grad=None,\n",
      "weight_decay=0.05,\n",
      "lr=None,\n",
      "blr=0.0005,\n",
      "layer_decay=0.65,\n",
      "min_lr=1e-06,\n",
      "warmup_epochs=5,\n",
      "color_jitter=None,\n",
      "aa='rand-m9-mstd0.5-inc1',\n",
      "smoothing=0.1,\n",
      "reprob=0.25,\n",
      "remode='pixel',\n",
      "recount=1,\n",
      "resplit=False,\n",
      "mixup=0.8,\n",
      "cutmix=1.0,\n",
      "cutmix_minmax=None,\n",
      "mixup_prob=1.0,\n",
      "mixup_switch_prob=0.5,\n",
      "mixup_mode='batch',\n",
      "finetune='mae_pretrain_vit_base.pth',\n",
      "global_pool=True,\n",
      "data_path='../data/ABGQI_mel_spectrograms',\n",
      "nb_classes=5,\n",
      "output_dir='./embeddings/mobilenetv4_r448_pretrained_16_bs64',\n",
      "log_dir='./output_dir',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='.',\n",
      "start_epoch=0,\n",
      "eval=True,\n",
      "dist_eval=False,\n",
      "num_workers=10,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=-1,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://')\n",
      "[16:07:24.254680] [16:07:24.254671] [16:07:24.254699] Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('VE extraction', add_help=False)\n",
    "parser.add_argument('--batch_size', default=batch_size, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "parser.add_argument('--epochs', default=50, type=int)\n",
    "parser.add_argument('--accum_iter', default=4, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "# Model parameters\n",
    "parser.add_argument('--model', default=model_name, type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    # Optimizer parameters\n",
    "parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "parser.add_argument('--blr', type=float, default=5e-4, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "parser.add_argument('--layer_decay', type=float, default=0.65,\n",
    "                        help='layer-wise lr decay from ELECTRA/BEiT')\n",
    "\n",
    "parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Augmentation parameters\n",
    "parser.add_argument('--color_jitter', type=float, default=None, metavar='PCT',\n",
    "                        help='Color jitter factor (enabled only when not using Auto/RandAug)')\n",
    "parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "\n",
    "    # * Random Erase params\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "    # * Mixup params\n",
    "parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "\n",
    "parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "parser.add_argument('--finetune', default='mae_pretrain_vit_base.pth',\n",
    "                        help='finetune from checkpoint')\n",
    "parser.add_argument('--global_pool', action='store_true')\n",
    "parser.set_defaults(global_pool=True)\n",
    "parser.add_argument('--cls_token', action='store_false', dest='global_pool',\n",
    "                        help='Use class token instead of global pool for classification')\n",
    "# Dataset parameters\n",
    "parser.add_argument('--data_path', default=data_path, type=str,\n",
    "                        help='dataset path')\n",
    "parser.add_argument('--nb_classes', default=5, type=int,\n",
    "                        help='number of the classification types')\n",
    "parser.add_argument('--output_dir', default=EXPERIMENT_NAME,\n",
    "                        help='path where to save, empty for no saving')\n",
    "parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "\n",
    "parser.add_argument('--device', default=device,\n",
    "                        help='device to use for training / testing')\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--resume', default=\".\",\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "parser.add_argument('--eval',default=True, action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation (recommended during training for faster monitor')\n",
    "parser.add_argument('--num_workers', default=10, type=int)\n",
    "parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "parser.add_argument('--local_rank', default=-1, type=int)\n",
    "parser.add_argument('--dist_on_itp', action='store_true')\n",
    "parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "device = torch.device(args.device)\n",
    "\n",
    "\n",
    "# set seeds\n",
    "misc.init_distributed_mode(args)\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf2c6a-7936-4643-91c2-760581f58b40",
   "metadata": {},
   "source": [
    "## Adapt dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b30f1f-7e58-4c80-9e4c-cfaf5dc974ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:07:24.814370] [16:07:24.814365] [16:07:24.814423] [16:07:24.814348] [16:07:24.814443] [16:07:24.814439] [16:07:24.814455] Dataset ImageFolder\n",
      "    Number of datapoints: 7814\n",
      "    Root location: ../data/ABGQI_mel_spectrograms/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)\n",
      "               MaybeToTensor()\n",
      "               Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      "           )\n",
      "[16:07:24.818972] [16:07:24.818968] [16:07:24.818993] [16:07:24.818955] [16:07:24.819009] [16:07:24.819006] [16:07:24.819021] Dataset ImageFolder\n",
      "    Number of datapoints: 850\n",
      "    Root location: ../data/ABGQI_mel_spectrograms/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bicubic, max_size=None, antialias=warn)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n",
      "[16:07:24.819326] [16:07:24.819322] [16:07:24.819344] [16:07:24.819313] [16:07:24.819360] [16:07:24.819357] [16:07:24.819372] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x716846ff9fd0>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = build_dataset(is_train=True, args=args)\n",
    "dataset_val = build_dataset(is_train=False, args=args)\n",
    "\n",
    "if True:  # args.distributed:\n",
    "        num_tasks = misc.get_world_size()\n",
    "        global_rank = misc.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "        if args.dist_eval:\n",
    "            if len(dataset_val) % num_tasks != 0:\n",
    "                print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n",
    "                      'This will slightly alter validation results as extra duplicate entries are added to achieve '\n",
    "                      'equal num of samples per-process.')\n",
    "            sampler_val = torch.utils.data.DistributedSampler(\n",
    "                dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=True)  # shuffle=True to reduce monitor bias\n",
    "        else:\n",
    "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "if global_rank == 0 and args.log_dir is not None and not args.eval:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "else:\n",
    "        log_writer = None\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bc5ba-eb25-432c-a98b-c57e4f6128df",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "381c68fd-d9a1-4b0b-83cc-9c053371ea52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                               | 0/122 [00:00<?, ?it/s]/home/sebastian/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:05<00:00, 23.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00, 11.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings(model, data_loader, save_path, device, preprocess=None,data_config=None, transforms=None):\n",
    "    embeddings_list = []\n",
    "    targets_list = []\n",
    "    total_batches = len(data_loader)\n",
    "    with torch.no_grad(), tqdm(total=total_batches) as pbar:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        model.to(device)\n",
    "        for images, targets in data_loader:\n",
    "            if preprocess:\n",
    "                print(\"required processing\")\n",
    "                images = preprocess(images).squeeze()\n",
    "                images = images.to(device)\n",
    "                embeddings = model(images)\n",
    "            if transforms: # for timm models\n",
    "                \n",
    "                # get model specific transforms (normalization, resize)\n",
    "                data_config = timm.data.resolve_model_data_config(model)\n",
    "                transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "                images = images.to(device)                \n",
    "                embeddings = model(transforms(images))# output is (batch_size, num_features) shaped tensor\n",
    "\n",
    "            embeddings_list.append(embeddings.cpu().detach().numpy())  # Move to CPU and convert to NumPy\n",
    "            targets_list.append(targets.numpy())  # Convert targets to NumPy\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Concatenate embeddings and targets from all batches\n",
    "    embeddings = np.concatenate(embeddings_list).squeeze()\n",
    "    targets = np.concatenate(targets_list)\n",
    "    num_embeddings = embeddings.shape[1]\n",
    "    column_names = [f\"feat_{i}\" for i in range(num_embeddings)]\n",
    "    column_names.append(\"label\")\n",
    "\n",
    "    embeddings_with_targets = np.hstack((embeddings, np.expand_dims(targets, axis=1)))\n",
    "\n",
    "    # Create a DataFrame with column names\n",
    "    df = pd.DataFrame(embeddings_with_targets, columns=column_names)\n",
    "    \n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "preprocess=None\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "# Extract embeddings for training data\n",
    "extract_embeddings(model, data_loader_train, f'{EXPERIMENT_NAME}/train_embeddings.csv', device, preprocess, data_config, transforms)\n",
    "    \n",
    "# Extract embeddings for validation data\n",
    "extract_embeddings(model, data_loader_val, f'{EXPERIMENT_NAME}/val_embeddings.csv', device, preprocess,data_config, transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational",
   "language": "python",
   "name": "foundational"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
